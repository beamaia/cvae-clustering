{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://github.com/ml-postech/GM-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import log\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create false image\n",
    "image = torch.randint(0, 255, (3, 3, 512, 512), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not from gm-vae, just want to uinderstand it better since every task has different encoder and decoder layers\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, kernel:int=3, stride:int=1, padding:int=0) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Conv2d(3, 32, kernel, stride, padding), # input image will be bx3x512x512, output bx32x508x508\n",
    "            nn.AvgPool2d(3, 2), # 2x2 avg pooling, TODO: decide if want to use max pooling output bx32x253x253\n",
    "            nn.ReLU(), # output bx32x253x253\n",
    "            nn.Conv2d(32, 64, kernel, stride, padding), # input bx32x253x253, output bx64x251x251\n",
    "            nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx64x125x125\n",
    "            nn.ReLU(), # output bx64x125x125\n",
    "            nn.Conv2d(64, 64, kernel, stride, padding), # input bx64x125x125, output bx64x123x123\n",
    "            nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx64x61x61\n",
    "            nn.ReLU(), # output bx64x61x61\n",
    "            nn.Flatten(), # output bx64*61*61 \n",
    "        )\n",
    "\n",
    "        self.layer_mean = nn.Linear(64*61*61, 2048) \n",
    "        self.layer_logvar = nn.Linear(64*61*61, 2048)   \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Calculating the output size of the encoder\n",
    "        [(W-F+2P)/S + 1]. [(512-3+0)/1 + 1] = 510 conv1\n",
    "        [(W-F)/S] + 1]. [(510-3)/2 + 1] = 254 pool1\n",
    "        [(W-F+2P)/S + 1]. [(253-3+)/1 + 1] = 252 conv2\n",
    "        [(W-F)/S] + 1]. [(251-3)/2 + 1] = 125 pool2\n",
    "        [(W-F+2P)/S + 1]. [(125-3+)/1 + 1] = 123 conv3\n",
    "        [(W-F)/S] + 1]. [(123-3)/2 + 1] = 61 pool3\n",
    "        \"\"\"\n",
    "\n",
    "    def resize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # resize image to 3x512x512\n",
    "        x = F.interpolate(x, size=(512, 512), mode='bilinear', align_corners=True)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.resize(x)\n",
    "        x = self.encoder(x)\n",
    "        mean = self.layer_mean(x)\n",
    "        logvar = self.layer_logvar(x)\n",
    "\n",
    "        return x, mean.unsqueeze(1), logvar.unsqueeze(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encode(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argmax(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0][:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = encoder.encode(image)\n",
    "# [(W-F+2P)/S + 1]. [(512-3+0)/1 + 1] = 510 conv1\n",
    "# [(W-F)/S] + 1]. [(510-3)/2 + 1] = 254 pool1\n",
    "# [(W-F+2P)/S + 1]. [(253-3+)/1 + 1] = 252 conv2\n",
    "# [(W-F)/S] + 1]. [(251-3)/2 + 1] = 125 pool2\n",
    "# [(W-F+2P)/S + 1]. [(125-3+0)/1 + 1] = 123 conv3\n",
    "# [(W-F)/S] + 1]. [(123-3)/2 + 1] = 61 pool3\n",
    "# [(W-F+2P)/S + 1]. [(61-3+0)/1 + 1] = 59 conv4\n",
    "# [(W-F)/S] + 1]. [(59-3)/2 + 1] = 29 pool4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "    (9): ReLU()\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (layer_mean): Linear(in_features=968256, out_features=2048, bias=True)\n",
       "  (layer_logvar): Linear(in_features=968256, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(output_image.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image[0, 30200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = None\n",
    "\n",
    "    def reparameterize(self, mean: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        # log var = (log var^2) / 2\n",
    "        # e ^ log var = e^(log var^2 / 2)\n",
    "        # e ^ log var = e^ (2 * (log var) / 2)\n",
    "        # e ^ log var = e^ (log var)\n",
    "        # e ^ log var = var\n",
    "        # var = var (this is why we use logvar instead of variation, it makes it possible for back propagation to know if a number is negative or positive)\n",
    "\n",
    "        eps = 0.5\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z), mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean, logvar = vae.encoder.encode(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([3, 238144]), mean shape: torch.Size([3, 1, 2048]), logvar shape: torch.Size([3, 1, 2048])\n",
      "type of x: <class 'torch.Tensor'>, type of mean: <class 'torch.Tensor'>, type of logvar: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"x shape: {x.shape}, mean shape: {mean.shape}, logvar shape: {logvar.shape}\")\n",
    "print(f\"type of x: {type(x)}, type of mean: {type(mean)}, type of logvar: {type(logvar)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = vae.reparameterize(mean, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: torch.Size([3, 1, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(f\"z shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, kernel:int=3, stride:int=1, padding:int=0) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2048, 64*61*61),\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, 61, 61)),  # Reshape to [batch_size, 128, 3, 3]\n",
    "            nn.ConvTranspose2d(64, 64, kernel, stride, padding),  # Reverse the convolutions of the encoder\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel, stride, padding),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel, stride, padding),  # Adjusted to match the original image channels\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "            # nn.Conv2d(3, 32, kernel, stride, padding), # input image will be bx3x512x512, output bx32x508x508\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, TODO: decide if want to use max pooling output bx32x253x253\n",
    "            # nn.ReLU(), # output bx32x253x253\n",
    "            # nn.Conv2d(32, 64, kernel, stride, padding), # input bx32x253x253, output bx64x251x251\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx64x125x125\n",
    "            # nn.ReLU(), # output bx64x125x125\n",
    "            # nn.Conv2d(64, 64, kernel, stride, padding), # input bx64x125x125, output bx64x123x123\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx64x61x61\n",
    "            # nn.ReLU(), # output bx64x61x61\n",
    "            # nn.Flatten(), # output bx64*61*61 \n",
    "\n",
    "    def decode(self, r_samples: torch.Tensor) -> torch.Tensor:\n",
    "        linear = nn.Linear(2048, 64*61*61)\n",
    "        relu1 = nn.ReLU()  # Activation function\n",
    "        unflatten = nn.Unflatten(dim=1, unflattened_size=(64, 3, 3))  # Reshape to [batch_size, 128, 3, 3\n",
    "        conv1 = nn.ConvTranspose2d(64, 64, self.kernel, self.stride, self.padding)  # Reverse the convolutions of the encode\n",
    "        relu2 = nn.ReLU()\n",
    "        conv2 = nn.ConvTranspose2d(64, 32, self.kernel, self.stride, self.padding)\n",
    "        relu3 = nn.ReLU()\n",
    "        conv3 = nn.ConvTranspose2d(32, 3, self.kernel, self.stride, self.padding)  # Adjusted to match the original image channel\n",
    "        sigmoid = nn.Sigmoid()\n",
    "\n",
    "        print(f\"r_samples shape: {r_samples.shape}\")\n",
    "        x = linear(r_samples)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = relu1(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = unflatten(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = conv1(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = relu2(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = conv2(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = relu3(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = conv3(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        x = sigmoid(x)\n",
    "        print(f\"x shape: {x.shape}\")\n",
    "        \n",
    "        reconstructed_images = []\n",
    "        for sample in r_samples:\n",
    "            x = self.decoder(sample.squeeze())  # Remove the singleton dimensions\n",
    "            reconstructed_images.append(x.unsqueeze(0))  # Add batch dimension back\n",
    "        return torch.cat(reconstructed_images, dim=0)\n",
    "    \n",
    "    def forward(self, r_samples: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decode(r_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_samples shape: torch.Size([3, 1, 2048])\n",
      "x shape: torch.Size([3, 1, 238144])\n",
      "x shape: torch.Size([3, 1, 238144])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [64, 3, 3] don't multiply up to the size of dim 1 (1) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 47\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[1;34m(self, r_samples)\u001b[0m\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m relu1(x)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m conv1(x)\n",
      "File \u001b[1;32md:\\cvae-clustering\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\cvae-clustering\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\cvae-clustering\\.venv\\lib\\site-packages\\torch\\nn\\modules\\flatten.py:141\u001b[0m, in \u001b[0;36mUnflatten.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflattened_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\cvae-clustering\\.venv\\lib\\site-packages\\torch\\_tensor.py:1282\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[1;34m(self, dim, sizes)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munflatten(dim, sizes, names)\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unflatten: Provided sizes [64, 3, 3] don't multiply up to the size of dim 1 (1) in the input tensor"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "decoder.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(128, 64, kernel, stride, padding),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.ConvTranspose2d(64, 32, kernel, stride, padding),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.ConvTranspose2d(32, 3, kernel, stride, padding),\n",
    "        #     nn.ReLU()\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, dist = output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_cal = Distribution(mean, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_samples = distribution_cal.rsample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logvar, _ = encoder.encode(image)\n",
    "print(\"Mean shape: \", mean.shape)\n",
    "print(\"Logvar shape: \", logvar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSample shape: \", r_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder = decoder.decode(r_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # nn.BatchNorm2d(3),\n",
    "            # nn.Conv2d(3, 32, kernel, stride, padding), # input image will be bx3x512x512, output bx32x508x508\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, TODO: decide if want to use max pooling output bx32x253x253\n",
    "            # nn.ReLU(), # output bx32x253x253\n",
    "            # nn.Conv2d(32, 64, kernel, stride, padding), # input bx32x253x253, output bx64x251x251\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx64x125x125\n",
    "            # nn.ReLU(), # output bx64x125x125\n",
    "            # nn.Conv2d(64, 128, kernel, stride, padding), # input bx64x125x125, output bx128x123x123\n",
    "            # nn.AvgPool2d(3, 2), # 2x2 avg pooling, output bx128x61x61\n",
    "            # nn.ReLU(), # output bx128x61x61\n",
    "            # nn.Flatten(), # output bx128*61*61\n",
    "            # nn.Linear(128*61*61, 2* 128*3*3) # output bx3*128*61*61 \n",
    "\n",
    "# pytorch transpose conv2d\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1, 3, 512, 512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
